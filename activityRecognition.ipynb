{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs for Blink Recognition Time Series Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows the example as shown in the following website:<br>\n",
    "https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pandas import read_csv\n",
    "from numpy import dstack, mean, std\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, TimeDistributed, Conv1D, MaxPooling1D, Flatten, ConvLSTM2D\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 1000\n",
    "shift_amount = 10\n",
    "poly_regression_degree = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic LSTM Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section is divided into the following sections:\n",
    "1. Load the Data\n",
    "2. Fit and Evaluate a Model\n",
    "3. Summarize Results\n",
    "4. Final Examination"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a channel, skip the first 6 rows, replace NaN with rolling average\n",
    "def load_channel(filepath, columnindex):\n",
    "  dataframe = read_csv(filepath, usecols=[columnindex], names=['voltage'], header=None, delim_whitespace=True)\n",
    "\n",
    "  mask = dataframe['voltage'].isna()  # Create Boolean mask for Nan values\n",
    "\n",
    "  x = np.arange(len(dataframe['voltage']))\n",
    "  y = dataframe['voltage'].values\n",
    "  p = Polynomial.fit(x[~mask], y[~mask], deg=poly_regression_degree)  # Fit polynomial regression model\n",
    "\n",
    "  dataframe.loc[mask, 'voltage'] = p(x[mask])  # Replace NaN values with polynomial regression model values\n",
    "\n",
    "  # Convert the one column DataFrame to a numpy array\n",
    "  data = dataframe.to_numpy().squeeze()\n",
    "\n",
    "  # Calculate the number of rows in the new DataFrame\n",
    "  n_rows = (len(data) - timesteps) // shift_amount + 1\n",
    "\n",
    "  # Initilize the new DataFrame\n",
    "  new_df = pd.DataFrame(np.zeros((n_rows, timesteps)))\n",
    "\n",
    "  # Fill the new DataFrame with shifted windows of data\n",
    "  for i in range(n_rows):\n",
    "      start = i * shift_amount\n",
    "      end = start + timesteps\n",
    "      new_df.iloc[i,:] = data[start:end]\n",
    "  \n",
    "  # print(dataframe)\n",
    "  # print(new_df)\n",
    "  return new_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all channels into a 3D array of [samples, timesteps, features]\n",
    "# samples = number of rows\n",
    "# timesteps = size of rolling window\n",
    "# features = number of channels\n",
    "def load_group(filename):\n",
    "  loaded = list()\n",
    "  num_channels = 1\n",
    "  with open(filename, 'r') as f:\n",
    "    first_line = f.readline()\n",
    "    num_channels = len(first_line.split())\n",
    "\n",
    "  for channel in range(num_channels):\n",
    "    data = load_channel(filename, channel)\n",
    "    loaded.append(data)\n",
    "  # stack group so that features are the 3rd dimension\n",
    "  loaded = dstack(loaded)\n",
    "  return loaded, num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_comments(val):\n",
    "\tif val.startswith('#'):\n",
    "\t\t\treturn 2\n",
    "\telse:\n",
    "\t\t\treturn 0\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(filename):\n",
    "\t# load input data\n",
    "\tX, num_channels = load_group(filename)\n",
    "\n",
    "\t# load class output\n",
    "\t# 1 = punch\n",
    "\t# 2 = retract\n",
    "\tfile = open(filename, 'r')\n",
    "\traw_y = pd.DataFrame(np.zeros(len(file.readlines())), columns=['label'])\n",
    "\tfile.seek(0)\n",
    "\tarmExtended = False\n",
    "\tindex = 0\n",
    "\tfor line in file:\n",
    "\t\tline = line.strip()\n",
    "\t\tif '#' in line:\n",
    "\t\t\t\tif armExtended:\n",
    "\t\t\t\t\t\traw_y.loc[index, 'label'] = 2\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t\traw_y.loc[index, 'label'] = 1\n",
    "\t\t\t\tarmExtended = not armExtended\n",
    "\t\tindex += 1\n",
    "\tfile.close()\n",
    "\t# Convert the one column DataFrame to a numpy array\n",
    "\tdata = raw_y.to_numpy().squeeze()\n",
    "\n",
    "\t # Calculate the number of rows in the new DataFrame\n",
    "\tn_rows = (len(data) - timesteps) // shift_amount + 1\n",
    "\n",
    "  # Initilize the new DataFrame\n",
    "\ty = pd.DataFrame(np.zeros(n_rows), columns=['label'])\n",
    "\tprint(y.shape)\n",
    "\n",
    "\tdetection_range = (int(timesteps * 0.1), int(timesteps * 0.4))\n",
    "\tstart_detection = detection_range[0]\n",
    "\tend_detection = detection_range[1]\n",
    "  # Fill the new DataFrame with shifted windows of data\n",
    "\tfor i in range(n_rows):\n",
    "\t\tstart = i * shift_amount\n",
    "\t\tend = start + timesteps\n",
    "\t\twindow = data[start:end]\n",
    "\t\tif np.any(window[start_detection:end_detection] != 0):\n",
    "\t\t\ty.loc[i, 'label'] = window[start_detection:end_detection][np.nonzero(window[start_detection:end_detection])][0]\n",
    "\t\telse:\n",
    "\t\t\ty.loc[i, 'label'] = 0\n",
    "\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(filename):\t\n",
    "\t# load data\n",
    "\tX, y = load_dataset_group(filename)\n",
    "\ttrain_size = int(len(X) * 0.7)\n",
    "  \n",
    "\t# split into train and test\n",
    "\ttrainX, testX = X[0:train_size], X[train_size:]\n",
    "\ttrainy, testy = y[0:train_size], y[train_size:]\n",
    " \n",
    "\tprint(trainX.shape, trainy.shape)\n",
    "\tprint(testX.shape, testy.shape)\n",
    " \n",
    "\t# # zero-offset class values\n",
    "\t# trainy = trainy - 1\n",
    "\t# testy = testy - 1\n",
    " \n",
    "\t# one hot encode y\n",
    "\ttrainy = to_categorical(trainy)\n",
    "\ttesty = to_categorical(testy)\n",
    "\tprint(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "\treturn trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and Evaluate a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit and evaluate a model\n",
    "# def evaluate_model(trainX, trainy, testX, testy):\n",
    "#   verbose, epochs, batch_size = 0, 15, 64\n",
    "#   n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "#   model = keras.Sequential([\n",
    "#     LSTM(100, input_shape=(n_timesteps,n_features), return_sequences=True),\n",
    "#     Dropout(0.2),\n",
    "#     LSTM(50),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(100, activation='relu'),\n",
    "#     Dropout(0.2),\n",
    "#     Dense(n_outputs, activation='softmax')\n",
    "#   ])\n",
    "#   model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#   # fit network\n",
    "#   early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "#   history = model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose, validation_split=0.2, callbacks=[early_stop])\n",
    "#   # plot learning curves\n",
    "#   plt.plot(history.history['accuracy'], label='train')\n",
    "#   plt.plot(history.history['val_accuracy'], label='test')\n",
    "#   plt.legend()\n",
    "#   plt.show()\n",
    "#   # evaluate model\n",
    "#   _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "#   return accuracy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "  print(scores)\n",
    "  m, s = mean(scores), std(scores)\n",
    "  print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run an experiment\n",
    "def run_experiment(repeats=5):\n",
    "  # load data\n",
    "  trainX, trainy, testX, testy = load_dataset(\"Data/ThrowingPunch-Frontal-Elbow-Outward-ClosedElbow.txt\")\n",
    "  # repeat experiment\n",
    "  scores = list()\n",
    "  for r in range(repeats):\n",
    "    score = evaluate_model(trainX, trainy, testX, testy)\n",
    "    score = score * 100.0\n",
    "    print('>#%d: %.3f' % (r+1, score))\n",
    "    scores.append(score)\n",
    "  # summarize results\n",
    "  summarize_results(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-LSTM Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "  # define model\n",
    "  verbose, epochs, batch_size = 0, 50, 64\n",
    "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "  # reshape data into time steps of sub-sequences\n",
    "  n_steps, n_length = 10, 100\n",
    "  trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "  testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "  # define model\n",
    "  model = keras.Sequential([\n",
    "    TimeDistributed(Conv1D(filters=64, kernel_size=3,\n",
    "              activation='relu'), input_shape=(None, n_length, n_features)),\n",
    "    TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')),\n",
    "    TimeDistributed(Dropout(0.5)),\n",
    "    TimeDistributed(MaxPooling1D(pool_size=2)),\n",
    "    TimeDistributed(Flatten()),\n",
    "    LSTM(100, return_sequences=True),\n",
    "    Dropout(0.5),\n",
    "    LSTM(100),\n",
    "    Dropout(0.5),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(n_outputs, activation='softmax')\n",
    "  ])\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "  # fit network\n",
    "  early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
    "  history = model.fit(trainX, trainy, epochs=epochs,\n",
    "            batch_size=batch_size, verbose=verbose, validation_split=0.2, callbacks=[early_stopping])\n",
    "  # evaluate model\n",
    "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "  return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115496, 1)\n",
      "(80847, 1000, 2) (80847, 1)\n",
      "(34649, 1000, 2) (34649, 1)\n",
      "(80847, 1000, 2) (80847, 3) (34649, 1000, 2) (34649, 3)\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "Epoch 7: early stopping\n",
      ">#1: 78.199\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Epoch 5: early stopping\n",
      ">#2: 78.799\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Epoch 15: early stopping\n",
      ">#3: 78.894\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "Epoch 7: early stopping\n",
      ">#4: 77.041\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Epoch 10: early stopping\n",
      ">#5: 77.439\n",
      "[78.19850444793701, 78.79881262779236, 78.89404892921448, 77.04118490219116, 77.43946313858032]\n",
      "Accuracy: 78.074% (+/-0.732)\n"
     ]
    }
   ],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvLSTM Network Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A further extension of the CNN LSTM idea is to perform the convolutions of the CNN (e.g. how the CNN reads the input sequence data) as part of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fit and evaluate a model\n",
    "# def evaluate_model(trainX, trainy, testX, testy):\n",
    "#   # define model\n",
    "#   verbose, epochs, batch_size = 'auto', 25, 64\n",
    "#   n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "#   # reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "#   n_steps, n_length = 10, 100\n",
    "#   trainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
    "#   testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
    "#   # define model\n",
    "#   model = keras.Sequential([\n",
    "#     ConvLSTM2D(filters=64, kernel_size=(3, 3),\n",
    "#               activation='relu', input_shape=(n_steps, 1, n_length, n_features), padding='same', return_sequences=True),\n",
    "#     ConvLSTM2D(filters=64, kernel_size=(3, 3),\n",
    "#               activation='relu', padding='same', return_sequences=True),\n",
    "#     ConvLSTM2D(filters=64, kernel_size=(3, 3),\n",
    "#               activation='relu', padding='same', return_sequences=False),\n",
    "#     Dropout(0.5),\n",
    "#     Flatten(),\n",
    "#     Dense(100, activation='relu'),\n",
    "#     Dense(n_outputs, activation='softmax')\n",
    "#   ])\n",
    "#   model.compile(loss='categorical_crossentropy',\n",
    "#                 optimizer='adam', metrics=['accuracy'])\n",
    "#   # fit network\n",
    "#   model.fit(trainX, trainy, epochs=epochs,\n",
    "#             batch_size=batch_size, verbose=verbose, use_multiprocessing=True)\n",
    "#   # evaluate model\n",
    "#   _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "#   return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License:\n",
    "# ========\n",
    "# Use of this dataset in publications must be acknowledged by referencing the following publication [1] \n",
    "\n",
    "# [1] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz.\n",
    "# A Public Domain Dataset for Human Activity Recognition Using Smartphones.\n",
    "# 21th European Symposium on Artificial Neural Networks, Computational Intelligence\n",
    "# and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013. \n",
    "\n",
    "# This dataset is distributed AS-IS and no responsibility implied or\n",
    "# explicit can be addressed to the authors or their institutions for\n",
    "# its use or misuse. Any commercial use is prohibited.\n",
    "\n",
    "# Other Related Publications:\n",
    "# ===========================\n",
    "# [2] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, Jorge L.\n",
    "# Reyes-Ortiz.  Energy Efficient Smartphone-Based Activity Recognition\n",
    "# using Fixed-Point Arithmetic. Journal of Universal Computer Science.\n",
    "# Special Issue in Ambient Assisted Living: Home Care.   Volume 19, Issue 9. May 2013\n",
    "\n",
    "# [3] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and\n",
    "# Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones\n",
    "# using a Multiclass Hardware-Friendly Support Vector Machine. 4th\n",
    "# International Workshop of Ambient Assited Living, IWAAL 2012,\n",
    "# Vitoria-Gasteiz, Spain, December 3-5, 2012. Proceedings. Lecture\n",
    "# Notes in Computer Science 2012, pp 216-223. \n",
    "\n",
    "# [4] Jorge Luis Reyes-Ortiz, Alessandro Ghio, Xavier Parra-Llanas,\n",
    "# Davide Anguita, Joan Cabestany, Andreu Catal√†. Human Activity and\n",
    "# Motion Disorder Recognition: Towards Smarter Interactive Cognitive\n",
    "# Environments. 21th European Symposium on Artificial Neural Networks,\n",
    "# Computational Intelligence and Machine Learning, ESANN 2013. Bruges,\n",
    "# Belgium 24-26 April 2013.  \n",
    "\n",
    "# ==================================================================================================\n",
    "# Jorge L. Reyes-Ortiz, Alessandro Ghio, Luca Oneto, Davide Anguita and Xavier Parra. November 2013."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1534846e4fcc353f8c5feb22b2d4f8b6e6f8be66008bac3fccdeb2b473060024"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
