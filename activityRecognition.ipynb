{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs for Blink Recognition Time Series Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows the steps as shown in the following website:<br>\n",
    "https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A volunteer was asked to blink and press a button at the same time. This LSTM model is trained to recognize the blink using the button press as a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pandas import read_csv\n",
    "from numpy import dstack, mean, std\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, TimeDistributed, Conv1D, MaxPooling1D, Flatten, ConvLSTM2D\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic LSTM Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section is divided into the following sections:\n",
    "1. Load the Data\n",
    "2. Fit and Evaluate a Model\n",
    "3. Summarize Results\n",
    "4. Final Examination"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 1000\n",
    "shift_amount = 50\n",
    "poly_regression_degree = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a channel, skip the first 6 rows, replace NaN with rolling average\n",
    "def load_channel(filepath, columnindex):\n",
    "  dataframe = read_csv(filepath, usecols=[columnindex], names=['voltage'], header=None, delim_whitespace=True)\n",
    "\n",
    "  mask = dataframe['voltage'].isna()  # Create Boolean mask for Nan values\n",
    "\n",
    "  x = np.arange(len(dataframe['voltage']))\n",
    "  y = dataframe['voltage'].values\n",
    "  p = Polynomial.fit(x[~mask], y[~mask], deg=poly_regression_degree)  # Fit polynomial regression model\n",
    "\n",
    "  dataframe.loc[mask, 'voltage'] = p(x[mask])  # Replace NaN values with polynomial regression model values\n",
    "\n",
    "  # Convert the one column DataFrame to a numpy array\n",
    "  data = dataframe.to_numpy().squeeze()\n",
    "\n",
    "  # Calculate the number of rows in the new DataFrame\n",
    "  n_rows = (len(data) - timesteps) // shift_amount + 1\n",
    "\n",
    "  # Initilize the new DataFrame\n",
    "  new_df = pd.DataFrame(np.zeros((n_rows, timesteps)))\n",
    "\n",
    "  # Fill the new DataFrame with shifted windows of data\n",
    "  for i in range(n_rows):\n",
    "      start = i * shift_amount\n",
    "      end = start + timesteps\n",
    "      new_df.iloc[i,:] = data[start:end]\n",
    "  \n",
    "  # print(dataframe)\n",
    "  # print(new_df)\n",
    "  return new_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23100, 1000)\n"
     ]
    }
   ],
   "source": [
    "var = load_channel(\"Data/ThrowingPunch-Frontal-Elbow-Outward-ClosedElbow.txt\", 0)\n",
    "print(var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all channels into a 3D array of [samples, timesteps, features]\n",
    "# samples = number of rows\n",
    "# timesteps = size of rolling window\n",
    "# features = number of channels\n",
    "def load_group(filename):\n",
    "  loaded = list()\n",
    "  num_channels = 1\n",
    "  with open(filename, 'r') as f:\n",
    "    first_line = f.readline()\n",
    "    num_channels = len(first_line.split())\n",
    "\n",
    "  for channel in range(num_channels):\n",
    "    data = load_channel(filename, channel)\n",
    "    loaded.append(data)\n",
    "  # stack group so that features are the 3rd dimension\n",
    "  loaded = dstack(loaded)\n",
    "  return loaded, num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var = load_group(\"Data/ThrowingPunch-Frontal-Elbow-Outward-ClosedElbow.txt\")\n",
    "# print(var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_comments(val):\n",
    "\tif val.startswith('#'):\n",
    "\t\t\treturn 2\n",
    "\telse:\n",
    "\t\t\treturn 0\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(filename):\n",
    "\t# load input data\n",
    "\tX, num_channels = load_group(filename)\n",
    "\n",
    "\t# load class output\n",
    "\t# 1 = punch\n",
    "\t# 2 = retract\n",
    "\tfile = open(filename, 'r')\n",
    "\traw_y = pd.DataFrame(np.zeros(len(file.readlines())), columns=['label'])\n",
    "\tfile.seek(0)\n",
    "\tarmExtended = False\n",
    "\tindex = 0\n",
    "\tfor line in file:\n",
    "\t\tline = line.strip()\n",
    "\t\tif '#' in line:\n",
    "\t\t\t\tif armExtended:\n",
    "\t\t\t\t\t\traw_y.loc[index, 'label'] = 2\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t\traw_y.loc[index, 'label'] = 1\n",
    "\t\t\t\tarmExtended = not armExtended\n",
    "\t\tindex += 1\n",
    "\tfile.close()\n",
    "\t# pd.set_option('display.max_columns', None)\n",
    "\t# pd.set_option('display.max_rows', None)\n",
    "\t# print(raw_y)\n",
    "\t# Convert the one column DataFrame to a numpy array\n",
    "\tdata = raw_y.to_numpy().squeeze()\n",
    "\n",
    "\t # Calculate the number of rows in the new DataFrame\n",
    "\tn_rows = (len(data) - timesteps) // shift_amount + 1\n",
    "\n",
    "  # Initilize the new DataFrame\n",
    "\ty = pd.DataFrame(np.zeros(n_rows), columns=['label'])\n",
    "\tprint(y.shape)\n",
    "\n",
    "\tdetection_range = (int(timesteps * 0.1), int(timesteps * 0.4))\n",
    "\tstart_detection = detection_range[0]\n",
    "\tend_detection = detection_range[1]\n",
    "  # Fill the new DataFrame with shifted windows of data\n",
    "\tfor i in range(n_rows):\n",
    "\t\tstart = i * shift_amount\n",
    "\t\tend = start + timesteps\n",
    "\t\twindow = data[start:end]\n",
    "\t\tif np.any(window[start_detection:end_detection] != 0):\n",
    "\t\t\ty.loc[i, 'label'] = window[start_detection:end_detection][np.nonzero(window[start_detection:end_detection])][0]\n",
    "\t\telse:\n",
    "\t\t\ty.loc[i, 'label'] = 0\n",
    "\t\t# y.loc[i, 'label'] = next((x for x in window[int(timesteps * 0.1):int(timesteps * 0.4)] if x != 0), None)\n",
    "\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varX, vary = load_dataset_group(\"Data/ThrowingPunch-Frontal-Elbow-Outward-ClosedElbow.txt\")\n",
    "# print(vary.loc[2999, 'label'], vary.shape)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# print(vary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(filename):\t\n",
    "\t# load data\n",
    "\tX, y = load_dataset_group(filename)\n",
    "\ttrain_size = int(len(X) * 0.8)\n",
    "  \n",
    "\t# split into train and test\n",
    "\ttrainX, testX = X[0:train_size], X[train_size:]\n",
    "\ttrainy, testy = y[0:train_size], y[train_size:]\n",
    " \n",
    "\tprint(trainX.shape, trainy.shape)\n",
    "\tprint(testX.shape, testy.shape)\n",
    " \n",
    "\t# # zero-offset class values\n",
    "\t# trainy = trainy - 1\n",
    "\t# testy = testy - 1\n",
    " \n",
    "\t# one hot encode y\n",
    "\ttrainy = to_categorical(trainy)\n",
    "\ttesty = to_categorical(testy)\n",
    "\tprint(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "\treturn trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23100, 1)\n",
      "(18480, 1000, 2) (18480, 1)\n",
      "(4620, 1000, 2) (4620, 1)\n",
      "(18480, 1000, 2) (18480, 3) (4620, 1000, 2) (4620, 3)\n"
     ]
    }
   ],
   "source": [
    "trainX, trainy, testX, testy = load_dataset(\"Data/ThrowingPunch-Frontal-Elbow-Outward-ClosedElbow.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and Evaluate a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "  verbose, epochs, batch_size = 0, 15, 64\n",
    "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "  model = keras.Sequential([\n",
    "    LSTM(100, input_shape=(n_timesteps,n_features)),\n",
    "    Dropout(0.5),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(n_outputs, activation='softmax')\n",
    "  ])\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  # fit network\n",
    "  model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "  # evaluate model\n",
    "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "  return accuracy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "  print(scores)\n",
    "  m, s = mean(scores), std(scores)\n",
    "  print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run an experiment\n",
    "def run_experiment(repeats=10):\n",
    "  # load data\n",
    "  trainX, trainy, testX, testy = load_dataset(\"Data/ThrowingPunch-Frontal-Elbow-Outward-ClosedElbow.txt\")\n",
    "  # repeat experiment\n",
    "  scores = list()\n",
    "  for r in range(repeats):\n",
    "    score = evaluate_model(trainX, trainy, testX, testy)\n",
    "    score = score * 100.0\n",
    "    print('>#%d: %.3f' % (r+1, score))\n",
    "    scores.append(score)\n",
    "  # summarize results\n",
    "  summarize_results(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23100, 1)\n",
      "(18480, 1000, 2) (18480, 1)\n",
      "(4620, 1000, 2) (4620, 1)\n",
      "(18480, 1000, 2) (18480, 3) (4620, 1000, 2) (4620, 3)\n",
      ">#1: 64.719\n",
      ">#2: 64.307\n",
      ">#3: 63.463\n",
      ">#4: 63.658\n",
      ">#5: 65.866\n",
      ">#6: 64.221\n",
      ">#7: 64.740\n",
      ">#8: 66.450\n",
      ">#9: 66.970\n",
      ">#10: 63.420\n",
      "[64.71861600875854, 64.30736184120178, 63.463205099105835, 63.658010959625244, 65.86580276489258, 64.22078013420105, 64.74025845527649, 66.45021438598633, 66.96969866752625, 63.41991424560547]\n",
      "Accuracy: 64.781% (+/-1.189)\n"
     ]
    }
   ],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-LSTM Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "  # define model\n",
    "  verbose, epochs, batch_size = 0, 25, 64\n",
    "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "  # reshape data into time steps of sub-sequences\n",
    "  n_steps, n_length = 10, 100\n",
    "  trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "  testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "  # define model\n",
    "  model = keras.Sequential([\n",
    "    TimeDistributed(Conv1D(filters=64, kernel_size=3,\n",
    "              activation='relu'), input_shape=(None, n_length, n_features)),\n",
    "    TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')),\n",
    "    TimeDistributed(Dropout(0.5)),\n",
    "    TimeDistributed(MaxPooling1D(pool_size=2)),\n",
    "    TimeDistributed(Flatten()),\n",
    "    LSTM(100),\n",
    "    Dropout(0.5),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(n_outputs, activation='softmax')\n",
    "  ])\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "  # fit network\n",
    "  model.fit(trainX, trainy, epochs=epochs,\n",
    "            batch_size=batch_size, verbose=verbose)\n",
    "  # evaluate model\n",
    "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "  return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23100, 1)\n",
      "(18480, 1000, 2) (18480, 1)\n",
      "(4620, 1000, 2) (4620, 1)\n",
      "(18480, 1000, 2) (18480, 3) (4620, 1000, 2) (4620, 3)\n",
      ">#1: 69.697\n",
      ">#2: 71.797\n",
      ">#3: 72.164\n",
      ">#4: 70.693\n",
      ">#5: 70.736\n",
      ">#6: 71.991\n",
      ">#7: 70.628\n",
      ">#8: 69.978\n",
      ">#9: 71.147\n",
      ">#10: 71.537\n",
      "[69.69696879386902, 71.79653644561768, 72.16449975967407, 70.69264054298401, 70.73593139648438, 71.99134230613708, 70.6277072429657, 69.97835636138916, 71.14718556404114, 71.53679728507996]\n",
      "Accuracy: 71.037% (+/-0.795)\n"
     ]
    }
   ],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvLSTM Network Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A further extension of the CNN LSTM idea is to perform the convolutions of the CNN (e.g. how the CNN reads the input sequence data) as part of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "  # define model\n",
    "  verbose, epochs, batch_size = 0, 25, 64\n",
    "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "  # reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "  n_steps, n_length = 10, 100\n",
    "  trainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
    "  testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
    "  # define model\n",
    "  model = keras.Sequential([\n",
    "    ConvLSTM2D(filters=64, kernel_size=(1, 3),\n",
    "              activation='relu', input_shape=(n_steps, 1, n_length, n_features)),\n",
    "    Dropout(0.5),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(n_outputs, activation='softmax')\n",
    "  ])\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "  # fit network\n",
    "  model.fit(trainX, trainy, epochs=epochs,\n",
    "            batch_size=batch_size, verbose=verbose)\n",
    "  # evaluate model\n",
    "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23100, 1)\n",
      "(18480, 1000, 2) (18480, 1)\n",
      "(4620, 1000, 2) (4620, 1)\n",
      "(18480, 1000, 2) (18480, 3) (4620, 1000, 2) (4620, 3)\n",
      ">#1: 69.697\n",
      ">#2: 70.866\n",
      ">#3: 71.580\n",
      ">#4: 69.307\n",
      ">#5: 70.455\n",
      ">#6: 71.753\n",
      ">#7: 69.589\n",
      ">#8: 68.312\n",
      ">#9: 71.623\n",
      ">#10: 69.784\n",
      "[69.69696879386902, 70.865797996521, 71.58008813858032, 69.3073570728302, 70.45454382896423, 71.75324559211731, 69.58874464035034, 68.31169128417969, 71.62337899208069, 69.78355050086975]\n",
      "Accuracy: 70.297% (+/-1.092)\n"
     ]
    }
   ],
   "source": [
    "run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License:\n",
    "# ========\n",
    "# Use of this dataset in publications must be acknowledged by referencing the following publication [1] \n",
    "\n",
    "# [1] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and Jorge L. Reyes-Ortiz.\n",
    "# A Public Domain Dataset for Human Activity Recognition Using Smartphones.\n",
    "# 21th European Symposium on Artificial Neural Networks, Computational Intelligence\n",
    "# and Machine Learning, ESANN 2013. Bruges, Belgium 24-26 April 2013. \n",
    "\n",
    "# This dataset is distributed AS-IS and no responsibility implied or\n",
    "# explicit can be addressed to the authors or their institutions for\n",
    "# its use or misuse. Any commercial use is prohibited.\n",
    "\n",
    "# Other Related Publications:\n",
    "# ===========================\n",
    "# [2] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, Jorge L.\n",
    "# Reyes-Ortiz.  Energy Efficient Smartphone-Based Activity Recognition\n",
    "# using Fixed-Point Arithmetic. Journal of Universal Computer Science.\n",
    "# Special Issue in Ambient Assisted Living: Home Care.   Volume 19, Issue 9. May 2013\n",
    "\n",
    "# [3] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra and\n",
    "# Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones\n",
    "# using a Multiclass Hardware-Friendly Support Vector Machine. 4th\n",
    "# International Workshop of Ambient Assited Living, IWAAL 2012,\n",
    "# Vitoria-Gasteiz, Spain, December 3-5, 2012. Proceedings. Lecture\n",
    "# Notes in Computer Science 2012, pp 216-223. \n",
    "\n",
    "# [4] Jorge Luis Reyes-Ortiz, Alessandro Ghio, Xavier Parra-Llanas,\n",
    "# Davide Anguita, Joan Cabestany, Andreu Catal√†. Human Activity and\n",
    "# Motion Disorder Recognition: Towards Smarter Interactive Cognitive\n",
    "# Environments. 21th European Symposium on Artificial Neural Networks,\n",
    "# Computational Intelligence and Machine Learning, ESANN 2013. Bruges,\n",
    "# Belgium 24-26 April 2013.  \n",
    "\n",
    "# ==================================================================================================\n",
    "# Jorge L. Reyes-Ortiz, Alessandro Ghio, Luca Oneto, Davide Anguita and Xavier Parra. November 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1534846e4fcc353f8c5feb22b2d4f8b6e6f8be66008bac3fccdeb2b473060024"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
